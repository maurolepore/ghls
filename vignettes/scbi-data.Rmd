---
title: "scbi-data"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This article shows how the ghr (GitHub-R) package can help you explore and gather data from multiple GitHub repositories.

## Motivation

As a collaborative researcher, your datasets may be scattered across multiple GitHub organizations and repositories. How can you easily find and access interesting datasets when you need them?

The ghr package helps you to navigate your GitHub files form R, using a syntax similar to that of `remotes::install_github()`. For example, this is a valid path to a GitHub directory: `"maurolepore/ghr/R@master" (Notice the structure `"owner/repo/subdir@branch"`).

## Example

Here we'll use data from the Smithsonian Conservation Biology Institute (SCBI) ForestGEO plot. They present their data at <https://scbi-forestgeo.github.io/SCBI-ForestGEO-Data/>:

> This is the public data portal for the SCBI ForestGEO plot, which points to archive locations for our various data products (some in this repository, many elsewhere).

SCBI datasets are scattered across multiple organizations and repositories. Here we'll use the ghr package to find and access some of those datasets.

```{r}
library(ghr)
```

We'll also use some other general purpose packages to iterate over multiple elements of a vector, to manipulate paths, and to read .csv files.

```{r}
library(purrr)
library(fs)
library(readr)
```

## Climate

Climate data from SCBI is stored in the GitHub organization "forestgeo", particularly in the repository "Climate". 

`ghr_ls()` lists GitHub directories in a way similar to how `fs::dir_ls()` lists local directories. Here it reveals one ".md" file and two folders:

```{r}
ghr_ls("forestgeo/Climate/Met_Station_Data/SCBI")
```

The folders contains multiple files, including some .csv files:

```{r}
ghr_ls("forestgeo/Climate/Met_Station_Data/SCBI/Front Royal weather station")
```

We can use a regular expression (`regexp`) to focus, for example, on .csv files:

```{r}
ghr_ls(
  "forestgeo/Climate/Met_Station_Data/SCBI/ForestGEO_met_station-SCBI", 
  regexp = "[.]csv$"
)
```

### Species list

In addition to climate data, SCBI has a number of species-list datasets. These datasets are in the GitHub organization "SCBI-ForestGEO", in the "SCBI-ForestGEO-Data" repository, and in the "species-lists" folder.

```{r}
species_lists <- "SCBI-ForestGEO/SCBI-ForestGEO-Data/species_lists"
# Get the last part of the path
subdirs <- path_file(ghr_ls(species_lists))
subdirs
```

Let's explore the .csv files in each sub directory. To reduce duplication we first create a vector `paths` giving the path to all of the sub directories we want to explore. Then we apply `ghr_ls()` to each element in `paths` and focus on .csv files only via `regexp`.

```{r}
paths <- path(species_lists, subdirs)
paths

map(paths, ghr_ls, regexp = "[.]csv$")
```

Instead of the file names we can show the first few rows of each file. This is a little more challenging because we need not the name but the "download_url" of each file. Here we first use `ghr_get()` to get the meta-data we need from GitHub, then we specifically pull the "download_url" associated to each element of the vector `paths`. To iterate over each `paths` element we use `purrr::map()`.

```{r}
download_urls <- paths %>% 
  map(ghr_get) %>% 
  map(ghr_pull, "download_url")
```

Now we focus on .csv files only.

```{r}
csv_url <- download_urls %>% 
  unlist() %>% 
  keep(~ grepl("[.]csv$", .x))
csv_url
```

And finally we use `readr::read_csv()` to read each dataset into R.

```{r}
csv_url %>% 
  map(~ head(read_csv(.x)))
```
